{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkcXXMw8ww2t"
      },
      "source": [
        "#Deep Learning Architectures Assignment 2\n",
        "The goal is to design and train a neural network for a regression task.\n",
        "Detailed requirements were presented during the last lecture.\n",
        "\n",
        "#Summary\n",
        "#Dataset\n",
        "* The dataset contains 2210 examples.\n",
        "* Each example consists of 2500 time steps.\n",
        "* In each time step, 8 values are recorded.\n",
        "* The result consists of two real numbers representing the coordinates.\n",
        "\n",
        "##TODO:\n",
        "* Split the dataset into three sets: test, validation, and training.\n",
        "* Choose an architecture and design the model.\n",
        "* Design the training procedure.\n",
        "* Present the results. \\\\\n",
        "// A correctly trained model should make predictions with an average error of < 2.\n",
        "\n",
        "##What will be evaluated:\n",
        "* Understanding of the topic, exploration of the dataset, justification of architecture choice [5 points]\n",
        "* Correctness of the training implementation [5 points]\n",
        "* Error obtained on test and validation data [5 points]\n",
        "* Presentation of the achieved results [5 points]\n",
        "\n",
        "\n",
        "##Extended task for extra points:\n",
        "* Design a model for noisy data.\n",
        "* To add noise, use the addNoise function.\n",
        "* Start tests with low noise: 0.01 or 0.001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "iAUUBld7wTiS"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def download_part(filename):\n",
        "  base_url = f\"https://github.com/pa-k/AGU/blob/main/assignment2/{filename}?raw=true\"\n",
        "  url = urlopen(base_url)\n",
        "  binary_data = url.read()\n",
        "  with open(filename,\"wb\") as f:\n",
        "    f.write(binary_data)\n",
        "\n",
        "def loadDataset():\n",
        "    parts = [\"DLAA2.0.pkl\", \"DLAA2.1.pkl\", \"DLAA2.2.pkl\", \"DLAA2.3.pkl\"]\n",
        "    cData = b''\n",
        "    for part in parts:\n",
        "        if not os.path.exists(part):\n",
        "          download_part(part)\n",
        "        with open(part, \"rb\") as f:\n",
        "            cData += pickle.load(f)\n",
        "    return pickle.loads(cData)\n",
        "\n",
        "def addNoise(input, noiseLevel=0.1):\n",
        "  shape = input.shape\n",
        "  noise = np.random.randn(*shape)*noiseLevel*np.max(input)\n",
        "  return input+noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obxK-NnJwfOi"
      },
      "source": [
        "##Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mTJ6PHhwee_",
        "outputId": "aa2ff93d-22e1-4c14-b681-4da4bf3d8f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2210, 2500, 8)\n",
            "(2210, 2)\n",
            "[23 14]\n"
          ]
        }
      ],
      "source": [
        "x, y = loadDataset()\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(y[1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETL1vvFzwiuo"
      },
      "source": [
        "# Your solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DACJmoKcvZvP"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "BPus7vdSwoSf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOMT3-N0yCu1"
      },
      "source": [
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "-0uLCry8yEfj"
      },
      "outputs": [],
      "source": [
        "total_samples = len(x)\n",
        "train_ratio=0.7\n",
        "val_ratio=0.15\n",
        "test_ratio=0.15\n",
        "\n",
        "train_size = int(total_samples * train_ratio)\n",
        "val_size = int(total_samples * val_ratio)\n",
        "test_size = total_samples - train_size - val_size\n",
        "\n",
        "# Create random indices for splitting\n",
        "indices = np.random.permutation(total_samples)\n",
        "\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:train_size + val_size]\n",
        "test_indices = indices[train_size + val_size:]\n",
        "\n",
        "x_train, y_train = x[train_indices], y[train_indices]\n",
        "x_val, y_val = x[val_indices], y[val_indices]\n",
        "x_test, y_test = x[test_indices], y[test_indices]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_normalized = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n",
        "x_val_normalized = scaler.transform(x_val.reshape(-1, x_val.shape[-1])).reshape(x_val.shape)\n",
        "x_test_normalized = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(x_train_normalized, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "x_val_tensor = torch.tensor(x_val_normalized, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aszAmfB5yf7y"
      },
      "source": [
        "Neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "26cUxJoXyh6E"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(2500 * 8, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, 128)\n",
        "        self.fc6 = nn.Linear(128, 64)\n",
        "        self.fc7 = nn.Linear(64, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(2048)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = self.fc7(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = RegressionModel()\n",
        "model.to(device)\n",
        "criterion = nn.SmoothL1Loss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "num_epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2n2lRYEyy6J"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAQS--MWy0KB",
        "outputId": "ead6366d-0305-4328-bb65-44023c74e60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Train Loss: 14.02995089461284, Val Loss: 12.751348544463651\n",
            "Epoch 2/100, Train Loss: 9.937538527332743, Val Loss: 12.866297678644923\n",
            "Epoch 3/100, Train Loss: 9.01783643388409, Val Loss: 10.711503668494094\n",
            "Epoch 4/100, Train Loss: 8.232139486918237, Val Loss: 9.861454543030154\n",
            "Epoch 5/100, Train Loss: 7.60980887681188, Val Loss: 13.00576137127833\n",
            "Epoch 6/100, Train Loss: 7.474961276199252, Val Loss: 13.618678176511091\n",
            "Epoch 7/100, Train Loss: 6.7907405864367885, Val Loss: 11.615652663470035\n",
            "Epoch 8/100, Train Loss: 7.105627055004788, Val Loss: 13.526687094812306\n",
            "Epoch 9/100, Train Loss: 6.495759350604678, Val Loss: 7.972949905337884\n",
            "Epoch 10/100, Train Loss: 5.99990347176116, Val Loss: 14.113315103997637\n",
            "Epoch 11/100, Train Loss: 6.195126850680528, Val Loss: 8.190086857429804\n",
            "Epoch 12/100, Train Loss: 5.960717901539325, Val Loss: 8.814315617264578\n",
            "Epoch 13/100, Train Loss: 5.7198909967270835, Val Loss: 8.471731727577048\n",
            "Epoch 14/100, Train Loss: 5.85635754949598, Val Loss: 7.580130833513427\n",
            "Epoch 15/100, Train Loss: 5.54037874347561, Val Loss: 7.735995050643506\n",
            "Epoch 16/100, Train Loss: 5.389584316618611, Val Loss: 7.772387081042518\n",
            "Epoch 17/100, Train Loss: 5.926158754919603, Val Loss: 8.951372132200488\n",
            "Epoch 18/100, Train Loss: 5.488834188149371, Val Loss: 9.050651620882155\n",
            "Epoch 19/100, Train Loss: 5.370488083894282, Val Loss: 7.225783940168306\n",
            "Epoch 20/100, Train Loss: 5.383146656815591, Val Loss: 8.009837136167773\n",
            "Epoch 21/100, Train Loss: 5.22514808786709, Val Loss: 9.114215924300456\n",
            "Epoch 22/100, Train Loss: 5.027745234557869, Val Loss: 9.469359671601357\n",
            "Epoch 23/100, Train Loss: 5.052259420531753, Val Loss: 8.923521667088627\n",
            "Epoch 24/100, Train Loss: 5.299579455148195, Val Loss: 5.082683653989948\n",
            "Epoch 25/100, Train Loss: 5.092214624112702, Val Loss: 7.171447281390879\n",
            "Epoch 26/100, Train Loss: 4.963124112460254, Val Loss: 6.5027609217202915\n",
            "Epoch 27/100, Train Loss: 4.975488789711911, Val Loss: 8.191996414495739\n",
            "Epoch 28/100, Train Loss: 4.929908944779084, Val Loss: 10.505165722434974\n",
            "Epoch 29/100, Train Loss: 4.867379537149637, Val Loss: 6.242532328176354\n",
            "Epoch 30/100, Train Loss: 4.727871204393482, Val Loss: 8.464257626375042\n",
            "Epoch 31/100, Train Loss: 4.7279025520443065, Val Loss: 6.142397909366112\n",
            "Epoch 32/100, Train Loss: 4.732970759262943, Val Loss: 7.579450630349335\n",
            "Epoch 33/100, Train Loss: 4.779932516501192, Val Loss: 8.018302894430938\n",
            "Epoch 34/100, Train Loss: 4.668574515973358, Val Loss: 5.504291089279774\n",
            "Epoch 35/100, Train Loss: 4.664168563749071, Val Loss: 6.184438546978815\n",
            "Epoch 36/100, Train Loss: 4.68595605398966, Val Loss: 5.8284859325950595\n",
            "Epoch 37/100, Train Loss: 4.575951975395237, Val Loss: 7.511649428537605\n",
            "Epoch 38/100, Train Loss: 4.655713416101397, Val Loss: 6.785021503886427\n",
            "Epoch 39/100, Train Loss: 4.550227875700287, Val Loss: 7.514074531566701\n",
            "Epoch 40/100, Train Loss: 4.509229740483112, Val Loss: 6.710041617338751\n",
            "Epoch 41/100, Train Loss: 4.60135568459726, Val Loss: 6.328150814033347\n",
            "Epoch 42/100, Train Loss: 4.720499449263254, Val Loss: 5.715936412984151\n",
            "Epoch 43/100, Train Loss: 4.544195597142501, Val Loss: 9.499287478513228\n",
            "Epoch 44/100, Train Loss: 4.287980110165991, Val Loss: 9.168177863982509\n",
            "Epoch 45/100, Train Loss: 4.4821029661853, Val Loss: 7.051853701427264\n",
            "Epoch 46/100, Train Loss: 4.328668857283491, Val Loss: 7.875883877457449\n",
            "Epoch 47/100, Train Loss: 4.292151555910215, Val Loss: 5.850557407223566\n",
            "Epoch 48/100, Train Loss: 4.336506686984298, Val Loss: 6.152711738632525\n",
            "Epoch 49/100, Train Loss: 4.542519582342778, Val Loss: 5.152553145618957\n",
            "Epoch 50/100, Train Loss: 4.3902816374839775, Val Loss: 6.89264137579235\n",
            "Epoch 51/100, Train Loss: 4.292847953924612, Val Loss: 5.982642731277604\n",
            "Epoch 52/100, Train Loss: 4.348929880508393, Val Loss: 5.676069442599202\n",
            "Epoch 53/100, Train Loss: 4.582911575541777, Val Loss: 9.379213840219549\n",
            "Epoch 54/100, Train Loss: 4.349696366187443, Val Loss: 5.261499240679323\n",
            "Epoch 55/100, Train Loss: 4.321698206659279, Val Loss: 7.2572393172457135\n",
            "Epoch 56/100, Train Loss: 4.46393346601559, Val Loss: 6.872044600748944\n",
            "Epoch 57/100, Train Loss: 4.240508945048356, Val Loss: 6.7441549243523635\n",
            "Epoch 58/100, Train Loss: 4.28154143195963, Val Loss: 5.690924127296379\n",
            "Epoch 59/100, Train Loss: 4.172023104634991, Val Loss: 6.496475430053526\n",
            "Epoch 60/100, Train Loss: 4.101371351487419, Val Loss: 5.6148884534115515\n",
            "Epoch 61/100, Train Loss: 4.297915449123962, Val Loss: 5.302516492832103\n",
            "Epoch 62/100, Train Loss: 4.179335947104555, Val Loss: 4.8801144617201695\n",
            "Epoch 63/100, Train Loss: 4.291083801617218, Val Loss: 5.189912986179133\n",
            "Epoch 64/100, Train Loss: 4.2926989583408135, Val Loss: 4.844212805756629\n",
            "Epoch 65/100, Train Loss: 4.290995465915436, Val Loss: 4.561001416059419\n",
            "Epoch 66/100, Train Loss: 4.025825843552119, Val Loss: 5.197288266841738\n",
            "Epoch 67/100, Train Loss: 3.9107612376838787, Val Loss: 4.572065031420428\n",
            "Epoch 68/100, Train Loss: 4.0339758536394905, Val Loss: 4.514628468683479\n",
            "Epoch 69/100, Train Loss: 4.000841815161874, Val Loss: 4.839368451397945\n",
            "Epoch 70/100, Train Loss: 3.94054789903785, Val Loss: 5.243615769907787\n",
            "Epoch 71/100, Train Loss: 4.048105965757956, Val Loss: 5.61651694666583\n",
            "Epoch 72/100, Train Loss: 4.161597063253214, Val Loss: 4.150586226195367\n",
            "Epoch 73/100, Train Loss: 4.051002241521784, Val Loss: 5.162763961492348\n",
            "Epoch 74/100, Train Loss: 3.960836893061938, Val Loss: 4.870130055620591\n",
            "Epoch 75/100, Train Loss: 4.048006771607945, Val Loss: 4.792086135944931\n",
            "Epoch 76/100, Train Loss: 3.9189934530178037, Val Loss: 4.67991369224387\n",
            "Epoch 77/100, Train Loss: 3.8537561127502995, Val Loss: 4.564540907695575\n",
            "Epoch 78/100, Train Loss: 3.91304509225166, Val Loss: 4.886083654766717\n",
            "Epoch 79/100, Train Loss: 3.9060900832424643, Val Loss: 5.439820103774978\n",
            "Epoch 80/100, Train Loss: 3.711987004406466, Val Loss: 5.52378748550876\n",
            "Epoch 81/100, Train Loss: 3.861359606115912, Val Loss: 5.894816503063789\n",
            "Epoch 82/100, Train Loss: 3.8020364802039355, Val Loss: 4.897955535761899\n",
            "Epoch 83/100, Train Loss: 3.911854243386385, Val Loss: 7.981823814599536\n",
            "Epoch 84/100, Train Loss: 4.035144424623416, Val Loss: 5.478668633544553\n",
            "Epoch 85/100, Train Loss: 3.8495947974068776, Val Loss: 4.7552651710740745\n",
            "Epoch 86/100, Train Loss: 3.827253020803776, Val Loss: 4.45109379759728\n",
            "Epoch 87/100, Train Loss: 3.720258847158342, Val Loss: 5.009247101325643\n",
            "Epoch 88/100, Train Loss: 3.7571395212554437, Val Loss: 6.61532406072242\n",
            "Epoch 89/100, Train Loss: 3.998443323023179, Val Loss: 4.749641721702415\n",
            "Epoch 90/100, Train Loss: 4.014789708291013, Val Loss: 4.213377661575364\n",
            "Epoch 91/100, Train Loss: 3.7677425364178077, Val Loss: 5.179486015412023\n",
            "Epoch 92/100, Train Loss: 3.8294400080759137, Val Loss: 5.902681703653941\n",
            "Epoch 93/100, Train Loss: 3.837480858517371, Val Loss: 5.031332613838403\n",
            "Epoch 94/100, Train Loss: 3.809154813799769, Val Loss: 4.7431724928657095\n",
            "Epoch 95/100, Train Loss: 4.00346158686652, Val Loss: 4.925485085144504\n",
            "Epoch 96/100, Train Loss: 3.8215505679191586, Val Loss: 6.159617174788184\n",
            "Epoch 97/100, Train Loss: 3.9150538925363234, Val Loss: 4.909648523589996\n",
            "Epoch 98/100, Train Loss: 3.6571719805194087, Val Loss: 3.912875476560564\n",
            "Epoch 99/100, Train Loss: 3.718267377299806, Val Loss: 4.475695388194658\n",
            "Epoch 100/100, Train Loss: 3.8667222068012337, Val Loss: 5.344480556303641\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Print training and validation loss\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader.dataset)}, Val Loss: {val_loss/len(val_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2hJrfwXzJSp"
      },
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOz0zSpSzKnL",
        "outputId": "deafe282-7cea-4143-f1f9-12d49dc97538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 5.4972\n"
          ]
        }
      ],
      "source": [
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        test_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
